{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7d4ddc",
   "metadata": {},
   "source": [
    "# Name: RAG\n",
    "\n",
    "## Description: Group Chat with Retrieval Augmented Generation.\n",
    "\n",
    "## Tags: RAG, FLAML\n",
    "\n",
    "###🧩 generated with ❤️ by Waldiez.\n",
    "\n",
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6f7b3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys  # pyright: ignore\n",
    "\n",
    "# # !{sys.executable} -m pip install -q ag2[openai]==0.9.6 beautifulsoup4 chromadb>=0.5.23 ipython markdownify protobuf==5.29.3 pypdf sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64081eb3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ff7e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# pyright: reportUnusedImport=false,reportMissingTypeStubs=false\n",
    "import csv\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "import sys\n",
    "from dataclasses import asdict\n",
    "from pprint import pprint\n",
    "from types import ModuleType\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Coroutine,\n",
    "    Dict,\n",
    "    List,\n",
    "    Optional,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import autogen  # type: ignore\n",
    "from autogen import (\n",
    "    Agent,\n",
    "    Cache,\n",
    "    ChatResult,\n",
    "    ConversableAgent,\n",
    "    GroupChat,\n",
    "    runtime_logging,\n",
    ")\n",
    "from autogen.agentchat import GroupChatManager, run_group_chat\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from autogen.agentchat.contrib.vectordb.chromadb import ChromaVectorDB\n",
    "from autogen.agentchat.group import ContextVariables\n",
    "from autogen.events import BaseEvent\n",
    "from autogen.io.run_response import AsyncRunResponseProtocol, RunResponseProtocol\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.embedding_functions.sentence_transformer_embedding_function import (\n",
    "    SentenceTransformerEmbeddingFunction,\n",
    ")\n",
    "\n",
    "#\n",
    "# let's try to avoid:\n",
    "# module 'numpy' has no attribute '_no_nep50_warning'\"\n",
    "# ref: https://github.com/numpy/numpy/blob/v2.2.2/doc/source/release/2.2.0-notes.rst#nep-50-promotion-state-option-removed\n",
    "os.environ[\"NEP50_DEPRECATION_WARNING\"] = \"0\"\n",
    "os.environ[\"NEP50_DISABLE_WARNING\"] = \"1\"\n",
    "os.environ[\"NPY_PROMOTION_STATE\"] = \"weak\"\n",
    "if not hasattr(np, \"_no_pep50_warning\"):\n",
    "\n",
    "    import contextlib\n",
    "    from typing import Generator\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def _np_no_nep50_warning() -> Generator[None, None, None]:\n",
    "        \"\"\"Dummy function to avoid the warning.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        None\n",
    "            Nothing.\n",
    "        \"\"\"\n",
    "        yield\n",
    "\n",
    "    setattr(np, \"_no_pep50_warning\", _np_no_nep50_warning)  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89c4d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Start logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_logging() -> None:\n",
    "    \"\"\"Start logging.\"\"\"\n",
    "    runtime_logging.start(\n",
    "        logger_type=\"sqlite\",\n",
    "        config={\"dbname\": \"flow.db\"},\n",
    "    )\n",
    "\n",
    "\n",
    "start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ca265",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Load model API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9adb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# This section assumes that a file named \"rag_api_keys\"\n",
    "# exists in the same directory as this file.\n",
    "# This file contains the API keys for the models used in this flow.\n",
    "# It should be .gitignored and not shared publicly.\n",
    "# If this file is not present, you can either create it manually\n",
    "# or change the way API keys are loaded in the flow.\n",
    "\n",
    "\n",
    "def load_api_key_module(flow_name: str) -> ModuleType:\n",
    "    \"\"\"Load the api key module.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flow_name : str\n",
    "        The flow name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ModuleType\n",
    "        The api keys loading module.\n",
    "    \"\"\"\n",
    "    module_name = f\"{flow_name}_api_keys\"\n",
    "    if module_name in sys.modules:\n",
    "        return importlib.reload(sys.modules[module_name])\n",
    "    return importlib.import_module(module_name)\n",
    "\n",
    "\n",
    "__MODELS_MODULE__ = load_api_key_module(\"rag\")\n",
    "\n",
    "\n",
    "def get_rag_model_api_key(model_name: str) -> str:\n",
    "    \"\"\"Get the model api key.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        The model name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The model api key.\n",
    "    \"\"\"\n",
    "    return __MODELS_MODULE__.get_rag_model_api_key(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d224fb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_1_llm_config: dict[str, Any] = {\n",
    "    \"model\": \"gpt-4.1\",\n",
    "    \"api_type\": \"openai\",\n",
    "    \"api_key\": get_rag_model_api_key(\"gpt_4_1\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d032c04",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2198278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnnecessaryIsInstance=false\n",
    "\n",
    "boss_assistant_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "boss_assistant_embedding_function = SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    ")\n",
    "boss_assistant_client.get_or_create_collection(\n",
    "    \"autogen-docs\",\n",
    "    embedding_function=boss_assistant_embedding_function,\n",
    ")\n",
    "\n",
    "boss_assistant = RetrieveUserProxyAgent(\n",
    "    name=\"boss_assistant\",\n",
    "    description=\"Assistant who has extra content retrieval power for solving difficult problems.\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    default_auto_reply=\"Reply 'TERMINATE' if the task is done.\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=lambda x: any(\n",
    "        isinstance(x, dict)\n",
    "        and x.get(\"content\", \"\")\n",
    "        and isinstance(x.get(\"content\", \"\"), str)\n",
    "        and x.get(\"content\", \"\").endswith(keyword)\n",
    "        for keyword in [\"TERMINATE\"]\n",
    "    ),\n",
    "    retrieve_config={\n",
    "        \"task\": \"default\",\n",
    "        \"model\": \"all-MiniLM-L6-v2\",\n",
    "        \"docs_path\": [\n",
    "            r\"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\"\n",
    "        ],\n",
    "        \"new_docs\": True,\n",
    "        \"update_context\": True,\n",
    "        \"get_or_create\": True,\n",
    "        \"overwrite\": False,\n",
    "        \"recursive\": True,\n",
    "        \"chunk_mode\": \"multi_lines\",\n",
    "        \"must_break_at_empty_line\": True,\n",
    "        \"collection_name\": \"autogen-docs\",\n",
    "        \"distance_threshold\": -1.0,\n",
    "        \"vector_db\": ChromaVectorDB(\n",
    "            client=boss_assistant_client,\n",
    "            embedding_function=boss_assistant_embedding_function,\n",
    "        ),\n",
    "        \"client\": boss_assistant_client,\n",
    "    },\n",
    "    llm_config=False,  # pyright: ignore\n",
    ")\n",
    "\n",
    "code_reviewer = ConversableAgent(\n",
    "    name=\"code_reviewer\",\n",
    "    description=\"Code Reviewer who can review the code.\",\n",
    "    system_message=\"You are a code reviewer. Reply 'TERMINATE' in the end when everything is done.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=lambda x: any(\n",
    "        isinstance(x, dict)\n",
    "        and x.get(\"content\", \"\")\n",
    "        and isinstance(x.get(\"content\", \"\"), str)\n",
    "        and x.get(\"content\", \"\").endswith(keyword)\n",
    "        for keyword in [\"TERMINATE\"]\n",
    "    ),\n",
    "    functions=[],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "product_manager = ConversableAgent(\n",
    "    name=\"product_manager\",\n",
    "    description=\"Product Manager who can design and plan the project.\",\n",
    "    system_message=\"You are a product manager. Reply 'TERMINATE' in the end when everything is done.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=lambda x: any(\n",
    "        isinstance(x, dict)\n",
    "        and x.get(\"content\", \"\")\n",
    "        and isinstance(x.get(\"content\", \"\"), str)\n",
    "        and x.get(\"content\", \"\").endswith(keyword)\n",
    "        for keyword in [\"TERMINATE\"]\n",
    "    ),\n",
    "    functions=[],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "senior_python_engineer = ConversableAgent(\n",
    "    name=\"senior_python_engineer\",\n",
    "    description=\"Senior Python Engineer\",\n",
    "    system_message=\"You are a senior python engineer, you provide python code to answer questions. Reply 'TERMINATE' in the end when everything is done.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=lambda x: any(\n",
    "        isinstance(x, dict)\n",
    "        and x.get(\"content\", \"\")\n",
    "        and isinstance(x.get(\"content\", \"\"), str)\n",
    "        and x.get(\"content\", \"\").endswith(keyword)\n",
    "        for keyword in [\"TERMINATE\"]\n",
    "    ),\n",
    "    functions=[],\n",
    "    update_agent_state_before_reply=[],\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "manager_group_chat = GroupChat(\n",
    "    agents=[product_manager, senior_python_engineer, code_reviewer, boss_assistant],\n",
    "    enable_clear_history=False,\n",
    "    send_introductions=False,\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    admin_name=\"boss_assistant\",\n",
    "    speaker_selection_method=\"round_robin\",\n",
    "    allow_repeat_speaker=True,\n",
    ")\n",
    "\n",
    "\n",
    "def callable_message_boss_assistant_to_manager(\n",
    "    sender: RetrieveUserProxyAgent,\n",
    "    recipient: ConversableAgent,\n",
    "    context: dict[str, Any],\n",
    ") -> Union[dict[str, Any], str]:\n",
    "    \"\"\"Get the message using the RAG message generator method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sender : RetrieveUserProxyAgent\n",
    "        The source agent.\n",
    "    recipient : ConversableAgent\n",
    "        The target agent.\n",
    "    context : dict[str, Any]\n",
    "        The context.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[dict[str, Any], str]\n",
    "        The message to send using the last carryover.\n",
    "    \"\"\"\n",
    "    carryover = context.get(\"carryover\", \"\")\n",
    "    if isinstance(carryover, list):\n",
    "        carryover = carryover[-1]\n",
    "    if not isinstance(carryover, str):\n",
    "        if isinstance(carryover, list):\n",
    "            carryover = carryover[-1]\n",
    "        elif isinstance(carryover, dict):\n",
    "            carryover = carryover.get(\"content\", \"\")\n",
    "    if not isinstance(carryover, str):\n",
    "        carryover = \"\"\n",
    "    message = sender.message_generator(sender, recipient, context)\n",
    "    if carryover:\n",
    "        message += carryover\n",
    "    return message\n",
    "\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    name=\"manager\",\n",
    "    description=\"The group manager agent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=None,\n",
    "    default_auto_reply=\"\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=None,  # pyright: ignore\n",
    "    groupchat=manager_group_chat,\n",
    "    llm_config=autogen.LLMConfig(\n",
    "        config_list=[\n",
    "            gpt_4_1_llm_config,\n",
    "        ],\n",
    "        cache_seed=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def get_sqlite_out(dbname: str, table: str, csv_file: str) -> None:\n",
    "    \"\"\"Convert a sqlite table to csv and json files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dbname : str\n",
    "        The sqlite database name.\n",
    "    table : str\n",
    "        The table name.\n",
    "    csv_file : str\n",
    "        The csv file name.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(dbname)\n",
    "    query = f\"SELECT * FROM {table}\"  # nosec\n",
    "    try:\n",
    "        cursor = conn.execute(query)\n",
    "    except sqlite3.OperationalError:\n",
    "        conn.close()\n",
    "        return\n",
    "    rows = cursor.fetchall()\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "    data = [dict(zip(column_names, row)) for row in rows]\n",
    "    conn.close()\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        csv_writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(data)\n",
    "    json_file = csv_file.replace(\".csv\", \".json\")\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def stop_logging() -> None:\n",
    "    \"\"\"Stop logging.\"\"\"\n",
    "    runtime_logging.stop()\n",
    "    if not os.path.exists(\"logs\"):\n",
    "        os.makedirs(\"logs\")\n",
    "    for table in [\n",
    "        \"chat_completions\",\n",
    "        \"agents\",\n",
    "        \"oai_wrappers\",\n",
    "        \"oai_clients\",\n",
    "        \"version\",\n",
    "        \"events\",\n",
    "        \"function_calls\",\n",
    "    ]:\n",
    "        dest = os.path.join(\"logs\", f\"{table}.csv\")\n",
    "        get_sqlite_out(\"flow.db\", table, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713bbb3d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Start chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1f15e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main(on_event: Optional[Callable[[BaseEvent], bool]] = None) -> RunResponseProtocol:\n",
    "    \"\"\"Start chatting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RunResponseProtocol\n",
    "        The result of the chat session, which can be a single ChatResult,\n",
    "        a list of ChatResults, or a dictionary mapping integers to ChatResults.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If the chat session fails.\n",
    "    \"\"\"\n",
    "    with Cache.disk(cache_seed=42) as cache:  # pyright: ignore\n",
    "        results = boss_assistant.run(\n",
    "            manager,\n",
    "            cache=cache,\n",
    "            summary_method=\"last_msg\",\n",
    "            clear_history=True,\n",
    "            problem=\"How to use spark for parallel training in FLAML? Give me sample code.\",\n",
    "            message=callable_message_boss_assistant_to_manager,\n",
    "        )\n",
    "        if on_event:\n",
    "            if not isinstance(results, list):\n",
    "                results = [results]\n",
    "            for index, result in enumerate(results):\n",
    "                for event in result.events:\n",
    "                    try:\n",
    "                        should_continue = on_event(event)\n",
    "                    except Exception as e:\n",
    "                        raise RuntimeError(\"Error in event handler: \" + str(e)) from e\n",
    "                    if event.type == \"run_completion\":\n",
    "                        should_continue = False\n",
    "                    if not should_continue:\n",
    "                        break\n",
    "        else:\n",
    "            if not isinstance(results, list):\n",
    "                results = [results]\n",
    "            for result in results:\n",
    "                result.process()\n",
    "\n",
    "        stop_logging()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774378b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": false,
   "hide_notebook_metadata": true,
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
